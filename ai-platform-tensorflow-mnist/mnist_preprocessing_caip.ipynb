{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this notebook\n",
    "[Google Cloud AI Platform Notebooks](https://cloud.google.com/ml-engine/docs/notebooks/overview) is a hosted JupyterLab environment that comes optimized for machine learning.  \n",
    "  \n",
    "### Instructions:\n",
    "1. [Set up your Google Cloud Platform (GCP) project](https://console.cloud.google.com/cloud-resource-manager?_ga=2.150499254.-1267767919.1550615012).\n",
    "2. [Enable billing for the GCP project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "3. [Enable the Compute Engine API.](https://console.cloud.google.com/flows/enableapi?apiid=compute.googleapis.com&_ga=2.150499254.-1267767919.1550615012)\n",
    "4. [Create a new AI Platform Notebooks instance.](https://cloud.google.com/ml-engine/docs/notebooks/create-new)\n",
    "    - Select \"TensorFlow 1.x\" as the instance type, or ML framework.\n",
    "    - Including a GPU for this tutorial is not necessary. However, it may be helpful for future (or existing SageMaker) models that do require GPUs. [TODO: add support for using GPU]\n",
    "5. Select \"Open Jupyterlab\" for thte "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess MNIST dataset\n",
    "Convert the MNIST images into TFRecords and upload the TFRecords to Google Cloud Storage (GCS).\n",
    "  \n",
    "## Install libraries\n",
    "AI Platform Notebooks comes pre-installed with TensorFlow and Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Google Cloud Storage\n",
    "When working with AI Platform, it is recommended to store TFRecords in GCS. More information on working with GCS with AI Platform can be found [here.](https://cloud.google.com/ml-engine/docs/tensorflow/working-with-cloud-storage) \n",
    "   \n",
    "Specify a name for your existing (or new) GCS bucket with the BUCKET_NAME. It should be prefixed with \"gs://\" and must be unique across all buckets in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME='gs://ml-model-migration'\n",
    "REGION='us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the GCS bucket must be created, run the following bash command. Creating a GCS bucket can either be done through the front-end or command line. More instructions on creating a Google Cloud Storage Bucket can be found [here.](https://cloud.google.com/storage/docs/creating-buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://ml-model-migration/...\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l {REGION} {BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AI Platform notebook is authenticated as the default Compute Engine service account (unless otherwise specified at the time of notebook creation). This means that it should already have authorization to create new buckets and read/write from existing buckets. \n",
    "  \n",
    "If you are getting authorization errors, review the relevant service account's IAM permissions. If the storage bucket is not part of the same project as this Notebook, the Compute Engine service account may need to be granted access to the Cloud Storage bucket.  \n",
    "  \n",
    "To check which service account should be granted access, verify which service account is authenticated for this notebook. The service account should be included as the \"email\" field for the access token's info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"issued_to\": \"111616252376478783342\",\n",
      "  \"audience\": \"111616252376478783342\",\n",
      "  \"scope\": \"https://www.googleapis.com/auth/userinfo.email https://www.googleapis.com/auth/cloud-platform\",\n",
      "  \"expires_in\": 3138,\n",
      "  \"email\": \"946556229441-compute@developer.gserviceaccount.com\",\n",
      "  \"verified_email\": true,\n",
      "  \"access_type\": \"offline\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def access_token():\n",
    "    return subprocess.check_output(\n",
    "        'gcloud auth application-default print-access-token',\n",
    "        shell=True,\n",
    "    ).decode().strip()\n",
    "\n",
    "!curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token={access_token()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the [Google Cloud Storage Python Client](https://github.com/googleapis/google-cloud-python/tree/master/storage), some Python modules support reading/writing files locally and with GCS interchangeably. The module will read/write from GCS if the \"gs://\" prefix for the file or directory is specified.   \n",
    "  \n",
    "Options include:\n",
    "- [tf.io.gfile](https://www.tensorflow.org/api_docs/python/tf/io/gfile) for file I/O wrappers without thread locking\n",
    "- [tf.io.TFRecordWriter](https://www.tensorflow.org/api_docs/python/tf/io/TFRecordWriter) for writing records to a TFRecords file in GCS\n",
    "- [pandas 0.24.0 or later](https://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be no differences between Google Cloud AI Platform and AWS Sagemaker for the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():   \n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = np.reshape(x_train, [-1, 28,28,1])\n",
    "    x_test = np.reshape(x_test, [-1, 28,28,1])\n",
    "    train_data = {'images':x_train, 'labels':y_train}\n",
    "    test_data = {'images':x_test, 'labels':y_test}\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tfrecords(data_set, name, directory):\n",
    "    \"\"\"Converts MNIST dataset to tfrecords.\n",
    "    \n",
    "    Args:\n",
    "        data_set: Dictionary containing a numpy array of images and labels.\n",
    "        name: Name given to the exported tfrecord dataset.\n",
    "        directory: Directory that the tfrecord files will be saved in.\n",
    "    \"\"\"\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "    \n",
    "    images = data_set['images']\n",
    "    labels = data_set['labels']\n",
    "    num_examples = images.shape[0]  \n",
    "    rows = images.shape[1]\n",
    "    cols = images.shape[2]\n",
    "    depth = images.shape[3]\n",
    "\n",
    "    filename = os.path.join(directory, name + '.tfrecords')\n",
    "    print('Writing', filename)\n",
    "   \n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    for index in range(num_examples):\n",
    "        image_raw = images[index].tostring()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'height': _int64_feature(rows),\n",
    "            'width': _int64_feature(cols),\n",
    "            'depth': _int64_feature(depth),\n",
    "            'label': _int64_feature(int(labels[index])),\n",
    "            'image_raw': _bytes_feature(image_raw)}))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gs://ml-model-migration/mnist_train.tfrecords\n"
     ]
    }
   ],
   "source": [
    "export_tfrecords(train_data, \"mnist_train\", BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
