{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this notebook\n",
    "[Google Cloud AI Platform Notebooks](https://cloud.google.com/ml-engine/docs/notebooks/overview) is a hosted JupyterLab environment that comes optimized for machine learning.  \n",
    "   \n",
    "Machine Learning in Google Cloud can be deployed from notebooks in GCP, local environments, or...\n",
    "  \n",
    "### Instructions for deploying Notebook on GCP:\n",
    "1. [Set up your Google Cloud Platform (GCP) project](https://console.cloud.google.com/cloud-resource-manager?_ga=2.150499254.-1267767919.1550615012).\n",
    "2. [Enable billing for the GCP project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "3. [Enable the Compute Engine API.](https://console.cloud.google.com/flows/enableapi?apiid=compute.googleapis.com&_ga=2.150499254.-1267767919.1550615012)\n",
    "4. [Create a new AI Platform Notebooks instance.](https://cloud.google.com/ml-engine/docs/notebooks/create-new)\n",
    "    - Select \"TensorFlow 1.x\" as the instance type, or ML framework.\n",
    "    - Including a GPU for this tutorial is not necessary. However, it may be helpful for future (or existing SageMaker) models that do require GPUs. [TODO: add support for using GPU]\n",
    "    - Select Python 3 if you are asked what type of kernel\n",
    "5. Select \"Open Jupyterlab\" for the new notebook. You will be redirected to a URL for your notebook instance.\n",
    "6. Clone this GitHub repository with the \"Git clone\" button in the notebook. [TODO: Find better link](https://cloud.google.com/ml-engine/docs/notebooks/save-to-github)  \n",
    "   \n",
    "Make sure to [shut down the Notebook](https://cloud.google.com/ml-engine/docs/notebooks/shut-down) when you're done with this tutorial to avoid any unnecessary charges. \n",
    "\n",
    "### Instructions for deploying Notebook locally:  \n",
    "If you're running this notebook outside of GCP, upload it like you normally do for Jupyter Notebooks.   \n",
    "   \n",
    "In order to access GCP services from a local Jupyter Notebook, you'll need to set up GCP authentication, so that your API requests can be authorized. This can be done by setting the Application Default Credentials:\n",
    "```\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/path/to/file.json\"\n",
    "```\n",
    "Setting the Application Default Credentials is not necessary when using AI Platform Notebooks unless you would like to authenticate using a different service account than the default Compute Engine service account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess MNIST dataset\n",
    "Convert the MNIST images into TFRecords and upload the TFRecords to Google Cloud Storage (GCS).\n",
    "  \n",
    "## Install libraries\n",
    "AI Platform Notebooks comes pre-installed with TensorFlow and Keras (if TensorFlow 1.x is set as the ML framework at creation time). [TODO]: determine if this section is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud AI Platform may come preinstalled a different version of TensorFlow than SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Google Cloud Storage\n",
    "When working with AI Platform, it is recommended to store training data in GCS. Reasons include:\n",
    "* Training data must be accessible to the training service\n",
    "* Storing data in GCS reduces latency\n",
    "\n",
    "More information on working with GCS with AI Platform can be found [here.](https://cloud.google.com/ml-engine/docs/tensorflow/working-with-cloud-storage) \n",
    "   \n",
    "Specify a name for your existing (or new) GCS bucket with the BUCKET_NAME. It should be prefixed with \"gs://\" and must be unique across all buckets in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME='gs://ml-model-migration'\n",
    "PROJECT='ml-model-migrations'\n",
    "REGION='us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new Storage Bucket\n",
    "If the GCS bucket must be created, run the following bash command. Creating a GCS bucket can either be done through the front-end or command line. More instructions on creating a Google Cloud Storage Bucket can be found [here.](https://cloud.google.com/storage/docs/creating-buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://ml-model-migration/...\n",
      "ServiceException: 409 Bucket ml-model-migration already exists.\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l {REGION} {BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication and Authorization\n",
    "The AI Platform notebook is authenticated as the default Compute Engine service account (unless otherwise specified at the time of notebook creation). This means that it should already have authorization to create new buckets and read/write from existing buckets. \n",
    "  \n",
    "If you are getting authorization errors, review the relevant service account's IAM permissions. If the storage bucket is not part of the same project as this Notebook, the Compute Engine service account may need to be granted access to the Cloud Storage bucket.  \n",
    "  \n",
    "To check which service account should be granted access, verify which service account is authenticated for this notebook. The service account should be included as the \"email\" field for the access token's info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"issued_to\": \"111616252376478783342\",\n",
      "  \"audience\": \"111616252376478783342\",\n",
      "  \"scope\": \"https://www.googleapis.com/auth/userinfo.email https://www.googleapis.com/auth/cloud-platform\",\n",
      "  \"expires_in\": 3584,\n",
      "  \"email\": \"946556229441-compute@developer.gserviceaccount.com\",\n",
      "  \"verified_email\": true,\n",
      "  \"access_type\": \"offline\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# This does not work when using ADC\n",
    "import subprocess\n",
    "\n",
    "def access_token():\n",
    "    return subprocess.check_output(\n",
    "        'gcloud auth application-default print-access-token',\n",
    "        shell=True,\n",
    "    ).decode().strip()\n",
    "\n",
    "!curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token={access_token()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Application Default Credentials:\n",
    "```\n",
    "!cat \"path/to/file.json\"\n",
    "```\n",
    "\n",
    "google.auth.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write and Upload TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the [Google Cloud Storage Python Client](https://github.com/googleapis/google-cloud-python/tree/master/storage), some Python modules support reading/writing files locally and with GCS interchangeably. The module will read/write from GCS if the \"gs://\" prefix for the file or directory is specified.   \n",
    "  \n",
    "Options include:\n",
    "- [tf.io.gfile](https://www.tensorflow.org/api_docs/python/tf/io/gfile) for file I/O wrappers without thread locking\n",
    "- [tf.io.TFRecordWriter](https://www.tensorflow.org/api_docs/python/tf/io/TFRecordWriter) for writing records to a TFRecords file in GCS\n",
    "- [pandas 0.24.0 or later](https://pandas.pydata.org/)  \n",
    "  \n",
    "Pandas also supports reading and writing files to S3. However, Pandas does not support creating TFRecords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SageMaker, writing TFRecords and then uploading them to cloud storage requires two seperate operations. TFRecords must first be written locally and then uploaded to S3. [TODO: confirm statement] In GCP, these actions can be done in a single step: TFRecords can be directly written in Google Cloud Storage using the aforementioned modules. \n",
    "\n",
    "It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():   \n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = np.reshape(x_train, [-1, 28, 28, 1])\n",
    "    x_test = np.reshape(x_test, [-1, 28, 28, 1])\n",
    "    train_data = {'images':x_train, 'labels':y_train}\n",
    "    test_data = {'images':x_test, 'labels':y_test}\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tfrecords(data_set, name, directory):\n",
    "    \"\"\"Converts MNIST dataset to tfrecords.\n",
    "    \n",
    "    Args:\n",
    "        data_set: Dictionary containing a numpy array of images and labels.\n",
    "        name: Name given to the exported tfrecord dataset.\n",
    "        directory: Directory that the tfrecord files will be saved in.\n",
    "    \"\"\"\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "    \n",
    "    images = data_set['images']\n",
    "    labels = data_set['labels']\n",
    "    num_examples = images.shape[0]  \n",
    "    rows = images.shape[1]\n",
    "    cols = images.shape[2]\n",
    "    depth = images.shape[3]\n",
    "\n",
    "    filename = os.path.join(directory, name + '.tfrecords')\n",
    "    print('Writing', filename)\n",
    "   \n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    for index in range(num_examples):\n",
    "        image_raw = images[index].tostring()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'height': _int64_feature(rows),\n",
    "            'width': _int64_feature(cols),\n",
    "            'depth': _int64_feature(depth),\n",
    "            'label': _int64_feature(int(labels[index])),\n",
    "            'image_raw': _bytes_feature(image_raw)}))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_tfrecords(train_data, 'tfrecord', os.path.join(BUCKET_NAME, 'train'))\n",
    "export_tfrecords(test_data, 'tfrecord', os.path.join(BUCKET_NAME, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating TFRecords with Apache Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this the first time! \n",
    "# !pip3 install --quiet apache-beam[gcp]\n",
    "# !pip3 install --quiet tensorflow-transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 02:09:19.508379 140616460195584 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow_transform/beam/common.py:51: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0718 02:09:19.521399 140616460195584 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow_transform/beam/impl.py:283: The name tf.SparseTensorValue is deprecated. Please use tf.compat.v1.SparseTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.beam import impl as tft_beam\n",
    "from tensorflow_transform.beam import tft_beam_io\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata, dataset_schema\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apache Beam](https://beam.apache.org/documentation/programming-guide/)....\n",
    "A Beam pipeline represents a Directed Acyclic Graph of steps. It can have multiple input sources or multiple output sinks; a Beam operation (PTransform) can both read and output multiple PCollections.\n",
    "\n",
    "Beam pipelines can be used with different runners, or engines.\n",
    "[Cloud Dataflow](https://cloud.google.com/dataflow/) is a fully-managed service which can be used to parallelize data preprocessing.   \n",
    "   \n",
    "A PCollection represents a distributed data set that your Beam pipeline operates on. Your pipeline typically [creates an initial PCollection](https://beam.apache.org/documentation/programming-guide/#creating-a-pcollection) by reading data from an external dataset or in-memory data. In order to distribute preprocessing, each data sample must be on its own row.  \n",
    "   \n",
    "In order for in-memory data to be read into a PCollection, each individual sample must be on its own row. So, we'll need to convert our MNIST data from a dict of lists (dictionary contains two keys, 'images' and 'labels', each with a list of values) to a list of dicts (list contains a dictionary for each sample, each dictionary containing 'image' and 'label').   \n",
    " \n",
    "Our samples will use the Python Beam, because most data scientists are familiar with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list_dicts(data_dict):\n",
    "    \"\"\"Convert dict of lists to list of dicts.\n",
    "    \n",
    "    Necessary to convert MNIST data so that each element of Beam PCollection\n",
    "    represents an individual sample.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range(len(data_dict['images'])):\n",
    "        element = {\n",
    "            'image': data_dict['images'][i],\n",
    "            'label': data_dict['labels'][i]\n",
    "        }\n",
    "        data_list.append(element)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = convert_to_list_dicts(train_data)\n",
    "test_data_list = convert_to_list_dicts(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create and run Beam pipeline locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam is a distributed batch and stream processing framework...   \n",
    "   \n",
    "Machine Learning can require lots of data. Preprocessing this data and generating TFRecords can be extremely compute intensive and take a lot of time. Apache Beam on Dataflow can drastically reduce the amount of time it will take to preprocess your data by distributing the work across multiple workers. \n",
    "\n",
    "Beam processes elements in random order.\n",
    "\n",
    "Code is the same for batch and for stream preprocessing.\n",
    "  \n",
    "For our toy dataset, it might take longer to generate TFRecords using Beam than using TFRecordWriter in the prior example. This is due to the additional overhead. As the preprocessing becomes more complex and training data scales, Beam will significantly reduce processing time.\n",
    "[TODO]: show time to run each (try with additional data)\n",
    "\n",
    "Beam pipelines require a source of data to process. You can either use an existing I/O connector or create your own:\n",
    "* [Built-in I/O Connectors](https://beam.apache.org/documentation/io/built-in/): to connect to Apache HDFS, Google Cloud Storage, local filesystems, BigQuery, etc.\n",
    "* [Create your own Beam source](https://beam.apache.org/documentation/io/developing-io-overview/): to connect to a data store that isn't supported by Beam's existing I/O connectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = PipelineOptions()\n",
    "temp_dir = os.path.join(BUCKET_NAME, 'temp')\n",
    "runner = 'DirectRunner' # Use DirectRunner to run pipeline locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs, rows, cols, depth):\n",
    "    \"\"\"Preprocesses \n",
    "    \n",
    "    Args rows, cols, depths are side inputs.\n",
    "    \"\"\"\n",
    "    image_raw = inputs['image'].tostring()\n",
    "    label = int(inputs['label'])\n",
    "    return {\n",
    "        'height':rows,\n",
    "        'width': cols,\n",
    "        'depth': depth,\n",
    "        'image_raw': image_raw,\n",
    "        'label': label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 02:09:31.726813 140616460195584 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow_transform/tf_metadata/schema_utils.py:63: The name tf.SparseFeature is deprecated. Please use tf.io.SparseFeature instead.\n",
      "\n",
      "W0718 02:09:31.728561 140616460195584 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/tensorflow_transform/tf_metadata/schema_utils.py:110: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    dataset_schema.from_feature_spec({\n",
    "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image_raw': tf.io.FixedLenFeature([], tf.string)\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 01:53:59.761785 140481446565632 tfrecordio.py:57] Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    }
   ],
   "source": [
    "sample_image = train_data_list[0]['image']\n",
    "rows = sample_image.shape[0]\n",
    "cols = sample_image.shape[1]\n",
    "depth = sample_image.shape[2]\n",
    "\n",
    "output_dir = os.path.join(BUCKET_NAME, 'data', datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\"))\n",
    "\n",
    "with beam.Pipeline(runner, options=options) as p:\n",
    "    with tft_beam.Context(temp_dir=temp_dir):\n",
    "        for dataset_type, dataset in [('Train', train_data_list),\n",
    "                                      ('Test', test_data_list)]:\n",
    "            input_data = (p \n",
    "                          | 'Create{}Data'.format(dataset_type) >> beam.Create(dataset)\n",
    "                          | 'Preprocess{}Data'.format(dataset_type) >> beam.Map(\n",
    "                              preprocessing_fn, rows, cols, depth))\n",
    "            \n",
    "            input_data | 'Write{}Data'.format(\n",
    "                dataset_type) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(output_dir, dataset_type),\n",
    "                    coder=tft.coders.ExampleProtoCoder(raw_data_metadata.schema))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Beam pipeline on Google Cloud Dataflow\n",
    "The same pipeline that you created to run Beam locally can be used on the Cloud with multiple workers.\n",
    "Distributed processing over multiple workers.\n",
    "\n",
    "The only differences between running locally and on the cloud \n",
    "\n",
    "Enable Dataflow API.\n",
    "You create a PCollection by either reading data from an external source using Beam’s Source API, or you can create a PCollection of data stored in an in-memory collection class in your driver program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = 'DataflowRunner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset_type):\n",
    "    from tensorflow.keras.datasets import mnist # import mnist in the package locally to ensure it's imported on workers\n",
    "    # alternatively, use full path for tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    (x, y) = (x_train, y_train) if dataset_type == 'Train' else (x_test, y_test)\n",
    "    for image, label in zip(x, y):\n",
    "        yield {'image': image, 'label': label}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import SetupOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'tensorflow-transform'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='preprocessing',\n",
    "    version='0.1',\n",
    "    author='Kim Milam',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you stop the cell after the job shows up in the Dataflow console, the job will continue running.   \n",
    "   \n",
    "Importing packages onto workers:\n",
    "* import package in ParDo module\n",
    "\n",
    "You should be able to easily replace the source and transformations in the below Beam Pipeline, so that it works with your data source and desired preprocessing steps.\n",
    "1. Create raw data and apply Python transformations\n",
    "2. Transform data into TFrecord format and apply TF transformations\n",
    "3. Serialize the TFrecord data and wrte to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 02:20:10.741082 140616460195584 pipeline_options.py:261] Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-6ec692cb-8757-44b0-8999-19b2bd5db10c.json']\n",
      "W0718 02:20:10.745774 140616460195584 pipeline_options.py:261] Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-6ec692cb-8757-44b0-8999-19b2bd5db10c.json']\n"
     ]
    }
   ],
   "source": [
    "sample_image = train_data_list[0]['image']\n",
    "rows = sample_image.shape[0]\n",
    "cols = sample_image.shape[1]\n",
    "depth = sample_image.shape[2]\n",
    "\n",
    "output_dir = os.path.join(BUCKET_NAME, 'data', datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\"))\n",
    "options = PipelineOptions()\n",
    "options.view_as(SetupOptions).setup_file = './setup.py'\n",
    "options.view_as(GoogleCloudOptions).project = PROJECT\n",
    "options.view_as(GoogleCloudOptions).job_name = 'job'+datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")\n",
    "options.view_as(GoogleCloudOptions).staging_location = os.path.join(BUCKET_NAME, 'staging')\n",
    "temp_dir = os.path.join(BUCKET_NAME, 'temp')\n",
    "options.view_as(GoogleCloudOptions).temp_location = temp_dir\n",
    "\n",
    "with beam.Pipeline(runner, options=options) as p:\n",
    "    with tft_beam.Context(temp_dir=temp_dir):\n",
    "        for dataset_type in ['Train', 'Test']: # iterate through dataset_types so PCollections stay separate\n",
    "            input_data = (p \n",
    "                          | 'Create{}'.format(dataset_type) >> beam.Create([dataset_type])\n",
    "                          | 'Read{}Data'.format(dataset_type) >> beam.FlatMap(read_data)\n",
    "                          | 'Preprocess{}Data'.format(dataset_type) >> beam.Map(preprocessing_fn, rows, cols, depth)\n",
    "                         )\n",
    "            input_data | 'Write{}Data'.format(\n",
    "                dataset_type) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(output_dir, dataset_type),\n",
    "                    coder=tft.coders.ExampleProtoCoder(raw_data_metadata.schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to make sure that all of the required packages are pickled and installed on the worker nodes.\n",
    "  \n",
    "  \n",
    "From Beam documentation:\n",
    "When you run your pipeline locally, the packages that your pipeline depends on are available because they are installed on your local machine. However, when you want to run your pipeline remotely, you must make sure these dependencies are available on the remote machines.\n",
    "\n",
    "By default, global imports, functions, and variables defined in the main session are not saved during the serialization of a Cloud Dataflow job. If, for example, your DoFns are defined in the main file and reference imports and functions in the global namespace, you can set the --save_main_session pipeline option to True. This will cause the state of the global namespace to be pickled and loaded on the Cloud Dataflow worker.\n",
    "\n",
    "We use the save_main_session option because one or more DoFn's in this\n",
    "workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: imports, functions and other variables defined in the global context of your __main__ file of your Dataflow pipeline are, by default, not available in the worker execution environment, and such references will cause a NameError, unless the --save_main_session pipeline option is set to True. Please see https://cloud.google.com/dataflow/faq#how-do-i-handle-nameerrors for additional documentation on configuring your worker execution environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Beam Pipeline</b>: Graph of transformations   \n",
    "<b>PTransform</b>: Transform performing massively parallel computation   \n",
    "<b>PCollection</b>: Data flowing in the pipeline   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrate data from AWS to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
