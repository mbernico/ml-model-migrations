{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this notebook\n",
    "[Google Cloud AI Platform Notebooks](https://cloud.google.com/ml-engine/docs/notebooks/overview) is a hosted JupyterLab environment that comes optimized for machine learning.  \n",
    "  \n",
    "### Instructions for deploying Notebook on GCP:\n",
    "1. [Set up your Google Cloud Platform (GCP) project](https://console.cloud.google.com/cloud-resource-manager?_ga=2.150499254.-1267767919.1550615012).\n",
    "2. [Enable billing for the GCP project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "3. [Enable the Compute Engine API.](https://console.cloud.google.com/flows/enableapi?apiid=compute.googleapis.com&_ga=2.150499254.-1267767919.1550615012)\n",
    "4. [Create a new AI Platform Notebooks instance.](https://cloud.google.com/ml-engine/docs/notebooks/create-new)\n",
    "    - Select \"TensorFlow 1.x\" as the instance type, or ML framework.\n",
    "    - Including a GPU for this tutorial is not necessary. However, it may be helpful for future (or existing SageMaker) models that do require GPUs. [TODO: add support for using GPU]\n",
    "5. Select \"Open Jupyterlab\" for the new notebook. You will be redirected to a URL for your notebook instance.\n",
    "6. Clone this GitHub repository with the \"Git clone\" button in the notebook. [TODO: Find better link](https://cloud.google.com/ml-engine/docs/notebooks/save-to-github)  \n",
    "   \n",
    "Make sure to [shut down the Notebook](https://cloud.google.com/ml-engine/docs/notebooks/shut-down) when you're done with this tutorial to avoid any unnecessary charges. \n",
    "\n",
    "### Instructions for deploying Notebook locally:  \n",
    "If you're running this notebook outside of GCP, upload it like you normally do for Jupyter Notebooks.   \n",
    "   \n",
    "In order to access GCP services from a local Jupyter Notebook, you'll need to set up authentication, so that your API requests can be authorized. This can be done by setting the Application Default Credentials:\n",
    "```\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/path/to/file.json\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess MNIST dataset\n",
    "Convert the MNIST images into TFRecords and upload the TFRecords to Google Cloud Storage (GCS).\n",
    "  \n",
    "## Install libraries\n",
    "AI Platform Notebooks comes pre-installed with TensorFlow and Keras (if TensorFlow 1.x is set as the ML framework at creation time). [TODO]: determine if this section is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud AI Platform may come preinstalled a different version of TensorFlow than SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Google Cloud Storage\n",
    "When working with AI Platform, it is recommended to store TFRecords in GCS. More information on working with GCS with AI Platform can be found [here.](https://cloud.google.com/ml-engine/docs/tensorflow/working-with-cloud-storage) \n",
    "   \n",
    "Specify a name for your existing (or new) GCS bucket with the BUCKET_NAME. It should be prefixed with \"gs://\" and must be unique across all buckets in Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME='gs://ml-model-migration'\n",
    "PROJECT='ml-model-migrations'\n",
    "REGION='us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new Storage Bucket\n",
    "If the GCS bucket must be created, run the following bash command. Creating a GCS bucket can either be done through the front-end or command line. More instructions on creating a Google Cloud Storage Bucket can be found [here.](https://cloud.google.com/storage/docs/creating-buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://ml-model-migration/...\n",
      "ServiceException: 409 Bucket ml-model-migration already exists.\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l {REGION} {BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication and Authorization\n",
    "The AI Platform notebook is authenticated as the default Compute Engine service account (unless otherwise specified at the time of notebook creation). This means that it should already have authorization to create new buckets and read/write from existing buckets. \n",
    "  \n",
    "If you are getting authorization errors, review the relevant service account's IAM permissions. If the storage bucket is not part of the same project as this Notebook, the Compute Engine service account may need to be granted access to the Cloud Storage bucket.  \n",
    "  \n",
    "To check which service account should be granted access, verify which service account is authenticated for this notebook. The service account should be included as the \"email\" field for the access token's info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"issued_to\": \"111616252376478783342\",\n",
      "  \"audience\": \"111616252376478783342\",\n",
      "  \"scope\": \"https://www.googleapis.com/auth/userinfo.email https://www.googleapis.com/auth/cloud-platform\",\n",
      "  \"expires_in\": 3242,\n",
      "  \"email\": \"946556229441-compute@developer.gserviceaccount.com\",\n",
      "  \"verified_email\": true,\n",
      "  \"access_type\": \"offline\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def access_token():\n",
    "    return subprocess.check_output(\n",
    "        'gcloud auth application-default print-access-token',\n",
    "        shell=True,\n",
    "    ).decode().strip()\n",
    "\n",
    "!curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token={access_token()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write and Upload TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the [Google Cloud Storage Python Client](https://github.com/googleapis/google-cloud-python/tree/master/storage), some Python modules support reading/writing files locally and with GCS interchangeably. The module will read/write from GCS if the \"gs://\" prefix for the file or directory is specified.   \n",
    "  \n",
    "Options include:\n",
    "- [tf.io.gfile](https://www.tensorflow.org/api_docs/python/tf/io/gfile) for file I/O wrappers without thread locking\n",
    "- [tf.io.TFRecordWriter](https://www.tensorflow.org/api_docs/python/tf/io/TFRecordWriter) for writing records to a TFRecords file in GCS\n",
    "- [pandas 0.24.0 or later](https://pandas.pydata.org/)  \n",
    "  \n",
    "Pandas also supports reading and writing files to S3. However, Pandas does not support creating TFRecords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SageMaker, writing TFRecords and then uploading them to cloud storage requires two seperate operations. TFRecords must first be written locally and then uploaded to S3. [TODO: confirm statement] In GCP, these actions can be done in a single step: TFRecords can be directly written in Google Cloud Storage using the aforementioned modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():   \n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = np.reshape(x_train, [-1, 28, 28, 1])\n",
    "    x_test = np.reshape(x_test, [-1, 28, 28, 1])\n",
    "    train_data = {'images':x_train, 'labels':y_train}\n",
    "    test_data = {'images':x_test, 'labels':y_test}\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tfrecords(data_set, name, directory):\n",
    "    \"\"\"Converts MNIST dataset to tfrecords.\n",
    "    \n",
    "    Args:\n",
    "        data_set: Dictionary containing a numpy array of images and labels.\n",
    "        name: Name given to the exported tfrecord dataset.\n",
    "        directory: Directory that the tfrecord files will be saved in.\n",
    "    \"\"\"\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "    def _bytes_feature(value):\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "    \n",
    "    images = data_set['images']\n",
    "    labels = data_set['labels']\n",
    "    num_examples = images.shape[0]  \n",
    "    rows = images.shape[1]\n",
    "    cols = images.shape[2]\n",
    "    depth = images.shape[3]\n",
    "\n",
    "    filename = os.path.join(directory, name + '.tfrecords')\n",
    "    print('Writing', filename)\n",
    "   \n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    for index in range(num_examples):\n",
    "        image_raw = images[index].tostring()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'height': _int64_feature(rows),\n",
    "            'width': _int64_feature(cols),\n",
    "            'depth': _int64_feature(depth),\n",
    "            'label': _int64_feature(int(labels[index])),\n",
    "            'image_raw': _bytes_feature(image_raw)}))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gs://ml-model-migration/mnist_train.tfrecords\n",
      "Writing gs://ml-model-migration/mnist_test.tfrecords\n"
     ]
    }
   ],
   "source": [
    "export_tfrecords(train_data, 'tfrecord', os.path.join(BUCKET_NAME, 'train'))\n",
    "export_tfrecords(test_data, 'tfrecord', os.path.join(BUCKET_NAME, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating TFRecords with Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.beam import impl as tft_beam\n",
    "from tensorflow_transform.beam import tft_beam_io\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata, dataset_schema\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list_dicts(data_dict):\n",
    "    \"\"\"Convert dict of lists to list of dicts.\n",
    "    \n",
    "    Necessary to convert MNIST data so that each element of Beam PCollection\n",
    "    represents an individual sample.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range(len(data_dict['images'])):\n",
    "        element = {\n",
    "            'image': data_dict['images'][i],\n",
    "            'label': data_dict['labels'][i]\n",
    "        }\n",
    "        data_list.append(element)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = convert_to_list_dicts(train_data)\n",
    "test_data_list = convert_to_list_dicts(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create and run Beam pipeline locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam is a distributed batch and stream processing framework...   \n",
    "   \n",
    "Machine Learning can require lots of data. Preprocessing this data and generating TFRecords can be extremely compute intensive and take a lot of time. Apache Beam on Dataflow can drastically reduce the amount of time it will take to preprocess your data by distributing the work across multiple workers. \n",
    "\n",
    "Beam processes elements in random order.\n",
    "\n",
    "Code is the same for batch and for stream preprocessing.\n",
    "  \n",
    "For our toy dataset, it might take longer to generate TFRecords using Beam than using TFRecordWriter in the prior example. This is due to the additional overhead. As the preprocessing becomes more complex and training data scales, Beam will significantly reduce processing time.\n",
    "[TODO]: show time to run each (try with additional data)\n",
    "\n",
    "Beam pipelines require a source of data to process. You can either use an existing I/O connector or create your own:\n",
    "* [Built-in I/O Connectors](https://beam.apache.org/documentation/io/built-in/): to connect to Apache HDFS, Google Cloud Storage, local filesystems, BigQuery, etc.\n",
    "* [Create your own Beam source](https://beam.apache.org/documentation/io/developing-io-overview/): to connect to a data store that isn't supported by Beam's existing I/O connectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = PipelineOptions()\n",
    "temp_dir = os.path.join(BUCKET_NAME, 'temp')\n",
    "runner = 'DirectRunner' # Use DirectRunner to run pipeline locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs, rows, cols, depth):\n",
    "    \"\"\"Preprocesses \n",
    "    \n",
    "    Args rows, cols, depths are side inputs.\n",
    "    \"\"\"\n",
    "    image_raw = inputs['image'].tostring()\n",
    "    label = int(inputs['label'])\n",
    "    return {\n",
    "        'height':rows,\n",
    "        'width': cols,\n",
    "        'depth': depth,\n",
    "        'image_raw': image_raw,\n",
    "        'label': label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    dataset_schema.from_feature_spec({\n",
    "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image_raw': tf.io.FixedLenFeature([], tf.string)\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-91d3b930124d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                           \u001b[0;34m|\u001b[0m \u001b[0;34m'Create{}Data'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                           | 'Preprocess{}Data'.format(dataset_type) >> beam.Map(\n\u001b[0;32m---> 15\u001b[0;31m                               preprocessing_fn, rows, cols, depth))\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#             input_data | 'Write{}Data'.format(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    424\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;31m# When possible, invoke a round trip through the runner API.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtest_runner_api\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_runner_api_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m       return Pipeline.from_runner_api(\n\u001b[1;32m    404\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_coders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m_verify_runner_api_compatible\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0mVisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVisitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mVisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, visitor)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0mvisited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvalueish\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, visitor, pipeline, visited)\u001b[0m\n\u001b[1;32m    816\u001b[0m       \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_composite_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m       \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleave_composite_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, visitor, pipeline, visited)\u001b[0m\n\u001b[1;32m    816\u001b[0m       \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_composite_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m       \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleave_composite_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, visitor, pipeline, visited)\u001b[0m\n\u001b[1;32m    819\u001b[0m       \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleave_composite_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m       \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;31m# Visit the outputs (one or more). It is essential to mark as visited the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mvisit_transform\u001b[0;34m(self, transform_node)\u001b[0m\n\u001b[1;32m    596\u001b[0m           \u001b[0;31m# Transforms must be picklable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m           pickler.loads(pickler.dumps(transform_node.transform,\n\u001b[0;32m--> 598\u001b[0;31m                                       enable_trace=False),\n\u001b[0m\u001b[1;32m    599\u001b[0m                         enable_trace=False)\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/internal/pickler.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(o, enable_trace)\u001b[0m\n\u001b[1;32m    237\u001b[0m   \u001b[0;31m# in-memory copies) and free up some possibly large and no-longer-needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m   \u001b[0;31m# memory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m   \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_image = train_data_list[0]['image']\n",
    "rows = sample_image.shape[0]\n",
    "cols = sample_image.shape[1]\n",
    "depth = sample_image.shape[2]\n",
    "\n",
    "output_dir = os.path.join(BUCKET_NAME, 'data', datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\"))\n",
    "\n",
    "with beam.Pipeline(runner, options=options) as p:\n",
    "    with tft_beam.Context(temp_dir=temp_dir):\n",
    "        for dataset_type, dataset in [('Train', train_data_list),\n",
    "                                      ('Test', test_data_list)]:\n",
    "            input_data = (p \n",
    "                          | 'Create{}Data'.format(dataset_type) >> beam.Create(dataset)\n",
    "                          | 'Preprocess{}Data'.format(dataset_type) >> beam.Map(\n",
    "                              preprocessing_fn, rows, cols, depth))\n",
    "            \n",
    "            input_data | 'Write{}Data'.format(\n",
    "                dataset_type) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(output_dir, dataset_type),\n",
    "                    coder=tft.coders.ExampleProtoCoder(raw_data_metadata.schema))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Beam pipeline on Google Cloud Dataflow\n",
    "The same pipeline that you created to run Beam locally can be used on the Cloud with multiple workers.\n",
    "Distributed processing over multiple workers.\n",
    "\n",
    "The only differences between running locally and on the cloud\n",
    "\n",
    "Enable Dataflow API.\n",
    "You create a PCollection by either reading data from an external source using Beamâ€™s Source API, or you can create a PCollection of data stored in an in-memory collection class in your driver program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = 'DataflowRunner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0713 00:55:47.574421 140556855891712 pipeline_options.py:261] Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-0a2f5d76-c82c-41eb-ba46-a19bb80a8621.json']\n",
      "W0713 00:55:47.579071 140556855891712 pipeline_options.py:261] Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-0a2f5d76-c82c-41eb-ba46-a19bb80a8621.json']\n"
     ]
    },
    {
     "ename": "DataflowRuntimeException",
     "evalue": "Dataflow pipeline failed. State: FAILED, Error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/apache_beam/internal/pickler.py\", line 254, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 317, in loads\n    return load(file, ignore)\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 305, in load\n    obj = pik.load()\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 474, in find_class\n    return StockUnpickler.find_class(self, module, name)\nImportError: No module named 'tensorflow_transform'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 649, in do_work\n    work_executor.execute()\n  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/executor.py\", line 176, in execute\n    op.start()\n  File \"apache_beam/runners/worker/operations.py\", line 577, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 578, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 579, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 217, in apache_beam.runners.worker.operations.Operation.start\n  File \"apache_beam/runners/worker/operations.py\", line 221, in apache_beam.runners.worker.operations.Operation.start\n  File \"apache_beam/runners/worker/operations.py\", line 526, in apache_beam.runners.worker.operations.DoOperation.setup\n  File \"apache_beam/runners/worker/operations.py\", line 531, in apache_beam.runners.worker.operations.DoOperation.setup\n  File \"/usr/local/lib/python3.5/site-packages/apache_beam/internal/pickler.py\", line 258, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 317, in loads\n    return load(file, ignore)\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 305, in load\n    obj = pik.load()\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 474, in find_class\n    return StockUnpickler.find_class(self, module, name)\nImportError: No module named 'tensorflow_transform'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataflowRuntimeException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-b62ae6cfe57b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfrecordio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWriteToTFRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     coder=tft.coders.ExampleProtoCoder(raw_data_metadata.schema))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    424\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         raise DataflowRuntimeException(\n\u001b[1;32m   1330\u001b[0m             \u001b[0;34m'Dataflow pipeline failed. State: %s, Error:\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m             (self.state, getattr(self._runner, 'last_error_msg', None)), self)\n\u001b[0m\u001b[1;32m   1332\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataflowRuntimeException\u001b[0m: Dataflow pipeline failed. State: FAILED, Error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/apache_beam/internal/pickler.py\", line 254, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 317, in loads\n    return load(file, ignore)\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 305, in load\n    obj = pik.load()\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 474, in find_class\n    return StockUnpickler.find_class(self, module, name)\nImportError: No module named 'tensorflow_transform'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 649, in do_work\n    work_executor.execute()\n  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/executor.py\", line 176, in execute\n    op.start()\n  File \"apache_beam/runners/worker/operations.py\", line 577, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 578, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 579, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 217, in apache_beam.runners.worker.operations.Operation.start\n  File \"apache_beam/runners/worker/operations.py\", line 221, in apache_beam.runners.worker.operations.Operation.start\n  File \"apache_beam/runners/worker/operations.py\", line 526, in apache_beam.runners.worker.operations.DoOperation.setup\n  File \"apache_beam/runners/worker/operations.py\", line 531, in apache_beam.runners.worker.operations.DoOperation.setup\n  File \"/usr/local/lib/python3.5/site-packages/apache_beam/internal/pickler.py\", line 258, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 317, in loads\n    return load(file, ignore)\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 305, in load\n    obj = pik.load()\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 474, in find_class\n    return StockUnpickler.find_class(self, module, name)\nImportError: No module named 'tensorflow_transform'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class read_data(beam.DoFn):\n",
    "    def process(self, dataset_type):\n",
    "        # from tensorflow.keras.datasets import mnist # import mnist in the package locally to ensure it's imported on workers\n",
    "        # alternatively, use full path for tf.keras.datasets.mnist.load_data()\n",
    "        \n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "        (x, y) = (x_train, y_train) if dataset_type == 'Train' else (x_test, y_test)\n",
    "        for image, label in zip(x, y):\n",
    "            yield {'image': image, 'label': label}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import SetupOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'dltk',\n",
    "    'nibabel',\n",
    "    'numpy>=1.14.2',\n",
    "    'pandas>=0.23.4',\n",
    "    'six',\n",
    "    'tensorflow-transform'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='preprocessing',\n",
    "    version='0.1',\n",
    "    author='Kim Milam',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you stop the cell after the job shows up in the Dataflow console, the job will continue running.   \n",
    "   \n",
    "Importing packages onto workers:\n",
    "* import package in ParDo module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0713 21:59:56.183981 140556855891712 pipeline_options.py:261] Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-0a2f5d76-c82c-41eb-ba46-a19bb80a8621.json']\n",
      "W0713 21:59:56.190254 140556855891712 pipeline_options.py:261] Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-0a2f5d76-c82c-41eb-ba46-a19bb80a8621.json']\n"
     ]
    },
    {
     "ename": "DataflowRuntimeException",
     "evalue": "Dataflow pipeline failed. State: FAILED, Error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 773, in run\n    self._load_main_session(self.local_staging_directory)\n  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 489, in _load_main_session\n    pickler.load_session(session_file)\n  File \"/usr/local/lib/python3.5/site-packages/apache_beam/internal/pickler.py\", line 280, in load_session\n    return dill.load_session(file_path)\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 410, in load_session\n    module = unpickler.load()\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 474, in find_class\n    return StockUnpickler.find_class(self, module, name)\nImportError: No module named 'tensorflow.python.util.deprecation_wrapper'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataflowRuntimeException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-8774677b941d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfrecordio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWriteToTFRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     coder=tft.coders.ExampleProtoCoder(raw_data_metadata.schema))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    424\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         raise DataflowRuntimeException(\n\u001b[1;32m   1330\u001b[0m             \u001b[0;34m'Dataflow pipeline failed. State: %s, Error:\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m             (self.state, getattr(self._runner, 'last_error_msg', None)), self)\n\u001b[0m\u001b[1;32m   1332\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataflowRuntimeException\u001b[0m: Dataflow pipeline failed. State: FAILED, Error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 773, in run\n    self._load_main_session(self.local_staging_directory)\n  File \"/usr/local/lib/python3.5/site-packages/dataflow_worker/batchworker.py\", line 489, in _load_main_session\n    pickler.load_session(session_file)\n  File \"/usr/local/lib/python3.5/site-packages/apache_beam/internal/pickler.py\", line 280, in load_session\n    return dill.load_session(file_path)\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 410, in load_session\n    module = unpickler.load()\n  File \"/usr/local/lib/python3.5/site-packages/dill/_dill.py\", line 474, in find_class\n    return StockUnpickler.find_class(self, module, name)\nImportError: No module named 'tensorflow.python.util.deprecation_wrapper'\n"
     ]
    }
   ],
   "source": [
    "rows = sample_image.shape[0]\n",
    "cols = sample_image.shape[1]\n",
    "depth = sample_image.shape[2]\n",
    "\n",
    "output_dir = os.path.join(BUCKET_NAME, 'data', datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\"))\n",
    "options = PipelineOptions()\n",
    "options.view_as(SetupOptions).setup_file = './setup.py'\n",
    "options.view_as(SetupOptions).save_main_session = True\n",
    "options.view_as(GoogleCloudOptions).project = PROJECT\n",
    "options.view_as(GoogleCloudOptions).job_name = 'job'+datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")\n",
    "options.view_as(GoogleCloudOptions).staging_location = os.path.join(BUCKET_NAME, 'staging')\n",
    "temp_dir = os.path.join(BUCKET_NAME, 'temp')\n",
    "options.view_as(GoogleCloudOptions).temp_location = temp_dir\n",
    "\n",
    "with beam.Pipeline(runner, options=options) as p:\n",
    "    with tft_beam.Context(temp_dir=temp_dir):\n",
    "        for dataset_type in ['Train', 'Test']: # iterate through dataset_types so PCollections stay separate\n",
    "            input_data = (p \n",
    "                          | 'Create{}'.format(dataset_type) >> beam.Create([dataset_type])\n",
    "                          | 'Read{}Data'.format(dataset_type) >> beam.ParDo(read_data())\n",
    "                          | 'Preprocess{}Data'.format(dataset_type) >> beam.Map(preprocessing_fn, rows, cols, depth)\n",
    "                         )\n",
    "            input_data | 'Write{}Data'.format(\n",
    "                dataset_type) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(output_dir, dataset_type),\n",
    "                    coder=tft.coders.ExampleProtoCoder(raw_data_metadata.schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to make sure that all of the required packages are pickled and installed on the worker nodes.\n",
    "  \n",
    "  \n",
    "From Beam documentation:\n",
    "When you run your pipeline locally, the packages that your pipeline depends on are available because they are installed on your local machine. However, when you want to run your pipeline remotely, you must make sure these dependencies are available on the remote machines.\n",
    "\n",
    "By default, global imports, functions, and variables defined in the main session are not saved during the serialization of a Cloud Dataflow job. If, for example, your DoFns are defined in the main file and reference imports and functions in the global namespace, you can set the --save_main_session pipeline option to True. This will cause the state of the global namespace to be pickled and loaded on the Cloud Dataflow worker.\n",
    "\n",
    "We use the save_main_session option because one or more DoFn's in this\n",
    "workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "setup_options.extra_packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: imports, functions and other variables defined in the global context of your __main__ file of your Dataflow pipeline are, by default, not available in the worker execution environment, and such references will cause a NameError, unless the --save_main_session pipeline option is set to True. Please see https://cloud.google.com/dataflow/faq#how-do-i-handle-nameerrors for additional documentation on configuring your worker execution environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Beam Pipeline</b>: Graph of transformations   \n",
    "<b>PTransform</b>: Transform performing massively parallel computation   \n",
    "<b>PCollection</b>: Data flowing in the pipeline   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
